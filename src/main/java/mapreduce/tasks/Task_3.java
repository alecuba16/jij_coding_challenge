package mapreduce.tasks;

import mapreduce.JobMapReduce;
import mapreduce.Utils;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.*;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import java.io.*;
import java.net.URI;
import java.util.*;

public class Task_3 extends JobMapReduce {
    public static final String TUPLE_SEPARATOR = "#";
    private static final int TUPLE_ADID_COUNT_POS = 0;
    private static final int TUPLE_IMPRESSIONS_POS = 1;
    private static final int TUPLE_SITENAME_POS = 2;

    /**
     * This function creates a string representation of a tuple of values. The tuple fields are separated by
     * the {@link Task_3#TUPLE_SEPARATOR} constant ({@link Task_3#TUPLE_SEPARATOR}).
     *
     * @param adIdCount A string variable containing the count value of the adId.
     * @param impressions A string variable with the number of impressions of an ad.
     * @param siteName The site name.
     * @return The tuple encoded as string with the parameters separated with {@link Task_3#TUPLE_SEPARATOR} as separator.
     */
    public static String createValueTuple(String adIdCount, String impressions, String siteName) {
        ArrayList<String> outputTuple = new ArrayList<>();
        outputTuple.add(TUPLE_ADID_COUNT_POS, adIdCount);
        outputTuple.add(TUPLE_IMPRESSIONS_POS, impressions);
        outputTuple.add(TUPLE_SITENAME_POS, siteName);
        return String.join(TUPLE_SEPARATOR, outputTuple);
    }


    public static long getAdIdCount(Text tuple) {
        String[] tupleFields = tuple.toString().split(TUPLE_SEPARATOR);
        return Long.parseLong(tupleFields[TUPLE_ADID_COUNT_POS]);
    }

    /**
     * This function is used to get the number of Impressions of a given tuple generated by {@link Task_3#createValueTuple}.
     *
     * @param tuple A tuple of data in the format of {@link Task_3#createValueTuple}.
     * @return A long value indicating the number of impressions for a given ad.
     */
    public static long getImpressionCount(Text tuple) {
        String[] tupleFields = tuple.toString().split(TUPLE_SEPARATOR);
        return Long.parseLong(tupleFields[TUPLE_IMPRESSIONS_POS]);
    }

    /**
     * This function is used to get the siteName of a given tuple generated by {@link Task_3#createValueTuple}.
     *
     * @param tuple A tuple of data in the format of {@link Task_3#createValueTuple}.
     * @return A String value with the siteName.
     */
    public static String getSiteName(Text tuple) {
        String[] tupleFields = tuple.toString().split(TUPLE_SEPARATOR);
        return tupleFields[TUPLE_SITENAME_POS];
    }

    public static class Task_3_Map extends Mapper<LongWritable, Text, Text, Text> {
        private Map<String, String> siteNames;

        public void fileToSiteNamesMap(Path filePath, String siteIdColumnName) throws IOException {
            try (BufferedReader bufferedReader =
                         new BufferedReader(new FileReader(filePath.toString()))) {
                String line;
                while ((line = bufferedReader.readLine()) != null) {
                    if (line.contains(siteIdColumnName)) continue;
                    String[] sitesArray = line.split(",");
                    String adsId = sitesArray[0];
                    String siteName = sitesArray[1];
                    siteNames.put(adsId, siteName);
                }
            }
        }

        @Override
        protected void setup(Context context) throws IOException, InterruptedException {
            super.setup(context);
            // Get the column names
            String siteIdColumnName = context.getConfiguration().getStrings("siteIdColumn")[0];
            siteNames = new HashMap<>();
            URI[] sitesFiles = context.getCacheFiles();
            if (sitesFiles != null && sitesFiles.length > 0) {
                for (URI sitesFile : sitesFiles) {
                    fileToSiteNamesMap(new Path(sitesFile.getPath()), siteIdColumnName);
                }
            }

        }

        @Override
        public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
            // The First row key has the columns, so we skip the first row
            if (key.get() == 0) return;

            // Get the column names
            String siteIdColumn = context.getConfiguration().getStrings("siteIdColumn")[0];
            String impressionsColumn = context.getConfiguration().getStrings("impressionsColumn")[0];

            // Split the data
            String[] arrayValues = value.toString().split(",");

            // Check if array is completed
            if ((arrayValues.length<4)) return;

            // Get the values
            String siteIdValue = Utils.getAttributeSiteAds(arrayValues, siteIdColumn);
            String impressionsValue = Utils.getAttributeSiteAds(arrayValues, impressionsColumn);

            // Check if keyColumnValue is null, skip the row
            if (siteIdValue == null) return;
            // If there is no impressions, then the number of impressions is 0
            if (impressionsValue == null) impressionsValue = "0";

            // Join operation
            String siteName="";
            if (siteNames.containsKey(siteIdValue)) {
                siteName = siteNames.get(siteIdValue);
            }
            // Create the output tuple with the adIdCount initialized to 1, this will be aggregated by the combiner
            String finalOutput = createValueTuple("1", impressionsValue, siteName);


            context.write(new Text(siteIdValue), new Text(finalOutput));
        }
    }

    public static class Task_3_Combiner extends Reducer<Text, Text, Text, Text> {
        @Override
        public void reduce(Text key, Iterable<Text> values, Context context) throws IOException, InterruptedException {
            // Counter variables
            long adidCount = 0;
            long impressionsSum = 0;
            String siteName = "";
            for (Text tuple : values) {
                adidCount += 1;
                impressionsSum += getImpressionCount(tuple);
                siteName = getSiteName(tuple);
            }
            // Prepare the output tuple with the format siteAds#adidCount#impressionsSum#siteName
            String finalOutput = createValueTuple(Long.toString(adidCount), Long.toString(impressionsSum), siteName);
            context.write(new Text(key), new Text(finalOutput));
        }
    }

    public static class Task_3_Reduce extends Reducer<Text, Text, Text, Text> {
        @Override
        public void reduce(Text key, Iterable<Text> values, Context context) throws IOException, InterruptedException {
            // Counter variables
            long adidCount = 0;
            long impressionsSum = 0;
            String siteName = "";
            // Accum the values
            for (Text tuple : values) {
                adidCount += getAdIdCount(tuple);
                siteName = getSiteName(tuple);
                impressionsSum += getImpressionCount(tuple);
            }

            // Calculate the average number of impressions per adId
            Double avgImpressionsBySite = adidCount != 0 ? (double) impressionsSum / adidCount : null;
            // Output will be the adId,revenue with one decimal
            String finalOutput = String.format("%.1f", avgImpressionsBySite);

            context.write(new Text(siteName), new Text(finalOutput));
        }

        // Override the run method for adding the CSV column header
        @Override
        public void setup(Context context) throws IOException, InterruptedException {
            // Get the value if write header is enabled
            boolean writeHeader = context.getConfiguration().getBoolean("writeHeader", true);
            if (writeHeader) {
                Text column = new Text("siteName");
                Text values = new Text("averageImpressions");
                context.write(column, values);
            }
        }
    }

    public Task_3() {
        this.input = null;
        this.output = null;
    }

    @Override
    public boolean run() throws IOException, ClassNotFoundException, InterruptedException {
        Configuration configuration = new Configuration();
        // Define the new job and the name it will be given
        Job job = Job.getInstance(configuration, "TASK_3");
        Task_3.configureJob(job, this.input, this.output);
        return job.waitForCompletion(true);
    }


    public static void configureJob(Job job, String[] pathIn, String pathOut) throws IOException {
        job.setJarByClass(Task_3.class);

        String sitesPath = pathIn[0];
        String sitesAdsPath = pathIn[1];
        // Configurations
        job.getConfiguration().set("mapred.textoutputformat.separator", ",");
        job.getConfiguration().setStrings("siteIdColumn", "siteId");
        job.getConfiguration().setStrings("impressionsColumn", "impressions");
        job.getConfiguration().setStrings("siteNameColumn", "siteName");

        FileInputFormat.setInputPaths(job, sitesAdsPath);

        // Set the mapper class
        job.setMapperClass(Task_3.Task_3_Map.class);

        // add files to cache
        File folder = new File(sitesPath);
        File[] listOfFiles = folder.listFiles();
        assert listOfFiles != null;
        for (File file : listOfFiles) {
            if (file.isFile()) {
                job.addCacheFile(file.toURI());
            }
        }


        // The mappers output classes
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(Text.class);

        // Set the combiner, is the same as the reducer but without the avg
        job.setCombinerClass(Task_3_Combiner.class);

        // Set the reducer class it must use
        job.setReducerClass(Task_3.Task_3_Reduce.class);

        // The output will be Text
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(Text.class);

        // Cleanup output path
        Path outputPath = new Path(pathOut);
        FileSystem fs = FileSystem.get(outputPath.toUri(), job.getConfiguration());
        fs.delete(outputPath, true);

        // The files the job will read from/write to
        FileOutputFormat.setOutputPath(job, new Path(pathOut));
    }
}
